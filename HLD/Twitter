400 million tweets a day flow through the system and it can take up to 5 minutes for a tweet to flow from Lady Gaga’s fingers to her 31 million followers.


Outliers, those with huge follower lists, are becoming a common case. Sending a tweet from a user with a lot of followers, that is with a large fanout, can be slow. Twitter tries to do it under 5 seconds, but it doesn’t always work, especially when celebrities tweet and tweet each other, which is happening more and more. One of the consequences is replies can arrive before the original tweet is received. Twitter is changing from doing all the work on writes to doing more work on reads for high value users. 

https://www.c-sharpcorner.com/article/fanout-design-with-rabbitmq-exchange/

http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html

FR
1 able to tweet
2 see other's tweets
3 


Pull based

    Targeted timeline. Things like twitter.com and home_timeline API. Tweets delivered to you because you asked to see them. Pull based delivery: you are requesting this data from Twitter via a REST API call.
    Query timeline. Search API. A query against the corpus. Return all the tweets that match a particular query as fast as you can.

Push based

    Twitter runs one of the largest real-time event systems pushing tweets at 22 MB/sec through the Firehose.
        Open a socket to Twitter and they will push all public tweets to you within 150 msec.
        At any given time there’s about 1 million sockets open to the push cluster.
        Goes to firehose clients like search engines. All public tweets go out these sockets. 


 Tweet comes in over a write API. It goes through load balancers and a TFE (Twitter Front End) and other stuff that won’t be addressed.
This is a very directed path. Completely precomputed home timeline. All the business rules get executed as tweets come in.
Immediately the fanout process occurs. Tweets that come in are placed into a massive Redis cluster. Each tweet is replicated 3 times on 3 different machines. At Twitter scale many machines fail a day.
Fanout queries the social graph service that is based on Flock. Flock maintains the follower and followings lists.

    Flock returns the social graph for a recipient and starts iterating through all the timelines stored in the Redis cluster.
    The Redis cluster has a couple of terabytes of RAM.
    Pipelined 4k destinations at a time
    Native list structure are used inside Redis.
    Let’s say you tweet and you have 20K followers. What the fanout daemon will do is look up the location of all 20K users inside the Redis cluster. Then it will start inserting the Tweet ID of the tweet into all those lists throughout the Redis cluster. So for every write of a tweet as many as 20K inserts are occurring across the Redis cluster.
    What is being stored is the tweet ID of the generated tweet, the user ID of the originator of the tweet, and 4 bytes of bits used to mark if it’s a retweet or a reply or something else.
    Your home timeline sits in a Redis cluster and is 800 entries long. If you page back long enough you’ll hit the limit. RAM is the limiting resource determining how long your current tweet set can be.
    Every active user is stored in RAM to keep latencies down.
    Active user is someone who has logged into Twitter within 30 days, which can change depending on cache capacity or Twitter’s usage.
    If you are not an active user then the tweet does not go into the cache.
    Only your home timelines hit disk.
    If you fall out of the Redis cluster then you go through a process called reconstruction.
        Query against the social graph service. Figure out who you follow. Hit disk for every single one of them and then shove them back into Redis.
        It’s MySQL handling disk storage via Gizzard, which abstracts away SQL transactions and provides global replication.
    By replicating 3 times if a machine has a problem then they won’t have to recreate the timelines for all the timelines on that machine per datacenter.
    If a tweet is actually a retweet then a pointer is stored to the original tweet.

When you query for your home timeline the Timeline Service is queried. The Timeline Service then only has to find one machine that has your home timeline on it.

    Effectively running 3 different hash rings because your timeline is in 3 different places.
    They find the first one they can get to fastest and return it as fast as they can.
    The tradeoff is fanout takes a little longer, but the read process is fast. About 2 seconds from a cold cache to the browser. For an API call it’s about 400 msec.

